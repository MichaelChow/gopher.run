---
title: "5.9 Muti-Agent"
date: 2025-08-01T01:50:00Z
draft: false
weight: 5009
---

# 5.9 Muti-Agent

## **‰∏Ä„ÄÅhost j**ournal

> üí° Multi Agent Á≥ªÁªüÁî±Â§ö‰∏™ÂçèÂêåÂ∑•‰ΩúÁöÑ Agent ÁªÑÊàêÔºåÊØè‰∏™ Agent ÈÉΩÊúâÂÖ∂ÁâπÂÆöÁöÑËÅåË¥£Âíå‰∏ìÈïø„ÄÇÈÄöËøá Agent Èó¥ÁöÑ‰∫§‰∫í‰∏éÂçè‰ΩúÔºåÂèØ‰ª•Â§ÑÁêÜÊõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÔºåÂÆûÁé∞ÂàÜÂ∑•Âçè‰Ωú„ÄÇËøôÁßçÊñπÂºèÁâπÂà´ÈÄÇÂêàÈúÄË¶ÅÂ§ö‰∏™‰∏ì‰∏öÈ¢ÜÂüüÁü•ËØÜÁªìÂêàÁöÑÂú∫ÊôØ„ÄÇ

**ÂÆûÈôÖÊ°à‰æãÔºöÊó•ËÆ∞Âä©Êâã Â§öAgent**

**Êû∂ÊûÑ‰ºòÂäø**

- **ËÅåË¥£ÂàÜÁ¶ª**Ôºö
    - **Host**¬†- Ë¥üË¥£ÊÑèÂõæËØÜÂà´ÔºåÂÜ≥ÂÆöË∞ÉÁî®Âì™‰∏™‰∏ìÂÆ∂
    - **Specialists**¬†-¬†‰∏ìÂÆ∂Êô∫ËÉΩ‰ΩìÔºåË¥üË¥£ÂÖ∑‰Ωì‰ªªÂä°ÊâßË°å
    - **Summarizer**¬†-¬†Ê±áÊÄªÂ§ö‰∏™‰∏ìÂÆ∂ÁöÑËæìÂá∫ÔºàÂèØÈÄâÔºâ
- **Ê®°ÂùóÂåñ**ÔºöÊØè‰∏™‰∏ìÂÆ∂ÂèØ‰ª•Áã¨Á´ãÂºÄÂèëÂíåÈÉ®ÁΩ≤
- **ÂèØÊâ©Â±ï**ÔºöÊòì‰∫éÊ∑ªÂä†Êñ∞ÁöÑ‰∏ìÂÆ∂
- **‰∏ì‰∏öÂåñ**ÔºöÊØè‰∏™‰∏ìÂÆ∂ÂèØ‰ª•ÈíàÂØπÁâπÂÆö‰ªªÂä°‰ºòÂåñ


**Â∑•‰ΩúÊµÅÁ®ãÔºö**

Áî®Êà∑ËæìÂÖ• ‚Üí HostÊÑèÂõæËØÜÂà´ ‚Üí Ë∑ØÁî±Âà∞‰∏ìÂÆ∂ ‚Üí ‰∏ìÂÆ∂ÊâßË°å ‚Üí Ê±áÊÄªÁªìÊûú ‚Üí ËøîÂõûÁî®Êà∑



**1. ÂàõÂª∫¬†Host**

- Host ‰ΩøÁî®Âº∫Â§ßÁöÑÊ®°ÂûãËøõË°åÊÑèÂõæËØÜÂà´
- SystemPrompt ÂÆö‰πâ‰∫Ü¬†Host ÁöÑËÅåË¥£ËåÉÂõ¥
- Host ‰ºöÂàÜÊûêÁî®Êà∑ËæìÂÖ•ÔºåÂÜ≥ÂÆöË∞ÉÁî®Âì™‰∏™‰∏ìÂÆ∂
```go
func newHost(ctx context.Context, baseURL, apiKey, modelName string) (*host.Host, error) {
    chatModel, err := openai.NewChatModel(ctx, &openai.ChatModelConfig{
        BaseURL: baseURL,
        Model:   modelName,
        ByAzure: true,
        APIKey:  apiKey,
    })
    if err != nil {
        return nil, err
    }

    return &host.Host{
        ChatModel:    chatModel,
        SystemPrompt: "You can read and write journal on behalf of the user. When user asks a question, always answer with journal content.",
    }, nil
}
```



1. **ÂàõÂª∫ÂÜôÊó•ËÆ∞‰∏ìÂÆ∂**
- ‰∏ìÂÆ∂‰ΩøÁî®‰∏ìÈó®ÁöÑÊ®°ÂûãÂíåÂèÇÊï∞
- ÈÄöËøá Chain ÁºñÊéíÂ§ÑÁêÜÊµÅÁ®ã
- ÂÆö‰πâÊòéÁ°ÆÁöÑ¬†AgentMeta ‰ø°ÊÅØ
```go
func newWriteJournalSpecialist(ctx context.Context) (*host.Specialist, error) {
    chatModel, err := ollama.NewChatModel(ctx, &ollama.ChatModelConfig{
        BaseURL: "http://localhost:11434",
        Model:   "llama3-groq-tool-use",
        Options: &api.Options{
            Temperature: 0.000001, // ‰ΩéÊ∏©Â∫¶Á°Æ‰øùËæìÂá∫Á®≥ÂÆö
        },
    })
    if err != nil {
        return nil, err
    }

    // ÂàõÂª∫Â§ÑÁêÜÈìæÔºöÈáçÂÜôÁî®Êà∑Êü•ËØ¢ ‚Üí ÂÜôÂÖ•Êñá‰ª∂
    chain := compose.NewChain[[]*schema.Message, *schema.Message]()
    
    // Á¨¨‰∏ÄÊ≠•ÔºöÈáçÂÜôÁî®Êà∑Êü•ËØ¢ÔºåÊèêÂèñÊó•ËÆ∞ÂÜÖÂÆπ
    chain.AppendLambda(compose.InvokableLambda(func(ctx context.Context, input []*schema.Message) ([]*schema.Message, error) {
        systemMsg := &schema.Message{
            Role:    schema.System,
            Content: "You are responsible for preparing the user query for insertion into journal. The user's query is expected to contain the actual text the user want to write to journal, as well as convey the intention that this query should be written to journal. You job is to remove that intention from the user query, while preserving as much as possible the user's original query, and output ONLY the text to be written into journal",
        }
        return append([]*schema.Message{systemMsg}, input...), nil
    })).
        AppendChatModel(chatModel).
        AppendLambda(compose.InvokableLambda(func(ctx context.Context, input *schema.Message) (string, error) {
            // ÂÜôÂÖ•Êñá‰ª∂
            now := time.Now()
            dateStr := now.Format("2006-01-02")
            filename := fmt.Sprintf("journal_%s.txt", dateStr)
            
            content := fmt.Sprintf("%s\n", input.Content)
            err := os.WriteFile(filename, []byte(content), 0644)
            if err != nil {
                return "", err
            }
            
            return fmt.Sprintf("Journal written successfully: %s", input.Content), nil
        }))

    r, err := chain.Compile(ctx)
    if err != nil {
        return nil, err
    }

    return &host.Specialist{
        AgentMeta: host.AgentMeta{
            Name:        "write_journal",
            IntendedUse: "write user's content to journal file",
        },
        Invokable: func(ctx context.Context, input []*schema.Message, opts ...agent.AgentOption) (*schema.Message, error) {
            return r.Invoke(ctx, input, agent.GetComposeOptions(opts...)...)
        },
    }, nil
}
```

**3. ÂàõÂª∫ËØªÊó•ËÆ∞‰∏ìÂÆ∂**

```go
func newReadJournalSpecialist(ctx context.Context) (*host.Specialist, error) {
    chatModel, err := ollama.NewChatModel(ctx, &ollama.ChatModelConfig{
        BaseURL: "http://localhost:11434",
        Model:   "llama3-groq-tool-use",
        Options: &api.Options{
            Temperature: 0.000001,
        },
    })
    if err != nil {
        return nil, err
    }

    // ÂàõÂª∫Â§ÑÁêÜÈìæÔºöËØªÂèñÊñá‰ª∂ ‚Üí Ê†ºÂºèÂåñËæìÂá∫
    chain := compose.NewChain[[]*schema.Message, *schema.Message]()
    chain.AppendLambda(compose.InvokableLambda(func(ctx context.Context, input []*schema.Message) (string, error) {
        now := time.Now()
        dateStr := now.Format("2006-01-02")
        filename := fmt.Sprintf("journal_%s.txt", dateStr)
        
        content, err := os.ReadFile(filename)
        if err != nil {
            if os.IsNotExist(err) {
                return "No journal entries found for today.", nil
            }
            return "", err
        }
        
        return string(content), nil
    }))

    r, err := chain.Compile(ctx)
    if err != nil {
        return nil, err
    }

    return &host.Specialist{
        AgentMeta: host.AgentMeta{
            Name:        "view_journal_content",
            IntendedUse: "read and display journal content",
        },
        Invokable: func(ctx context.Context, input []*schema.Message, opts ...agent.AgentOption) (*schema.Message, error) {
            return r.Invoke(ctx, input, agent.GetComposeOptions(opts...)...)
        },
    }, nil
}
```



**4. ÂàõÂª∫ÈóÆÁ≠î‰∏ìÂÆ∂**

```go
func newAnswerWithJournalSpecialist(ctx context.Context) (*host.Specialist, error) {
    chatModel, err := ollama.NewChatModel(ctx, &ollama.ChatModelConfig{
        BaseURL: "http://localhost:11434",
        Model:   "llama3-groq-tool-use",
        Options: &api.Options{
            Temperature: 0.000001,
        },
    })
    if err != nil {
        return nil, err
    }

    // ÂàõÂª∫ÂõæÔºöÂä†ËΩΩÊó•ËÆ∞ ‚Üí ÊèêÂèñÊü•ËØ¢ ‚Üí Ê®°Êùø ‚Üí Ê®°Âûã ‚Üí ÂõûÁ≠î
    graph := compose.NewGraph[[]*schema.Message, *schema.Message]()

    // Âä†ËΩΩÊó•ËÆ∞ËäÇÁÇπ
    if err = graph.AddLambdaNode("journal_loader", compose.InvokableLambda(func(ctx context.Context, input []*schema.Message) (string, error) {
        now := time.Now()
        dateStr := now.Format("2006-01-02")
        return loadJournal(dateStr)
    }), compose.WithOutputKey("journal")); err != nil {
        return nil, err
    }

    // ÊèêÂèñÊü•ËØ¢ËäÇÁÇπ
    if err = graph.AddLambdaNode("query_extractor", compose.InvokableLambda(func(ctx context.Context, input []*schema.Message) (string, error) {
        return input[len(input)-1].Content, nil
    }), compose.WithOutputKey("query")); err != nil {
        return nil, err
    }

    // ÂàõÂª∫Ê®°Êùø
    systemTpl := `Answer user's query based on journal content: {journal}`
    chatTpl := prompt.FromMessages(schema.FString,
        schema.SystemMessage(systemTpl),
        schema.UserMessage("{query}"),
    )
    if err = graph.AddChatTemplateNode("template", chatTpl); err != nil {
        return nil, err
    }

    if err = graph.AddChatModelNode("model", chatModel); err != nil {
        return nil, err
    }

    // ËøûÊé•ËäÇÁÇπ
    if err = graph.AddEdge("journal_loader", "template"); err != nil {
        return nil, err
    }
    if err = graph.AddEdge("query_extractor", "template"); err != nil {
        return nil, err
    }
    if err = graph.AddEdge("template", "model"); err != nil {
        return nil, err
    }
    if err = graph.AddEdge(compose.START, "journal_loader"); err != nil {
        return nil, err
    }
    if err = graph.AddEdge(compose.START, "query_extractor"); err != nil {
        return nil, err
    }
    if err = graph.AddEdge("model", compose.END); err != nil {
        return nil, err
    }

    r, err := graph.Compile(ctx)
    if err != nil {
        return nil, err
    }

    return &host.Specialist{
        AgentMeta: host.AgentMeta{
            Name:        "answer_with_journal",
            IntendedUse: "load journal content and answer user's question with it",
        },
        Invokable: func(ctx context.Context, input []*schema.Message, opts ...agent.AgentOption) (*schema.Message, error) {
            return r.Invoke(ctx, input, agent.GetComposeOptions(opts...)...)
        },
    }, nil
}
```

**5. ÁªÑË£Ö Multi-Agent**

```go
func main() {
    ctx := context.Background()
    
    // ÂàõÂª∫ Host
    h, err := newHost(ctx, "your_base_url", "your_api_key", "gpt-4")
    if err != nil {
        panic(err)
    }

    // ÂàõÂª∫‰∏ìÂÆ∂‰ª¨
    writer, err := newWriteJournalSpecialist(ctx)
    if err != nil {
        panic(err)
    }

    reader, err := newReadJournalSpecialist(ctx)
    if err != nil {
        panic(err)
    }

    answerer, err := newAnswerWithJournalSpecialist(ctx)
    if err != nil {
        panic(err)
    }

    // ÁªÑË£Ö Multi-Agent
    hostMA, err := host.NewMultiAgent(ctx, &host.MultiAgentConfig{
        Host: *h,
        Specialists: []*host.Specialist{
            writer,
            reader,
            answerer,
        },
    })
    if err != nil {
        panic(err)
    }

    // ÂàõÂª∫ÂõûË∞ÉÂ§ÑÁêÜÂô®
    cb := &logCallback{}

    // ‰∫§‰∫íÂæ™ÁéØ
    for {
        println("\n\nYou: ")

        var message string
        scanner := bufio.NewScanner(os.Stdin)
        for scanner.Scan() {
            message += scanner.Text()
            break
        }

        if err := scanner.Err(); err != nil {
            panic(err)
        }

        if message == "exit" {
            return
        }

        msg := &schema.Message{
            Role:    schema.User,
            Content: message,
        }

        // ÊµÅÂºèË∞ÉÁî®
        out, err := hostMA.Stream(ctx, []*schema.Message{msg}, host.WithAgentCallbacks(cb))
        if err != nil {
            panic(err)
        }

        defer out.Close()

        println("\nAnswer:")

        for {
            msg, err := out.Recv()
            if err != nil {
                if err == io.EOF {
                    break
                }
            }

            print(msg.Content)
        }
    }
}
```



**È´òÁ∫ßÈÖçÁΩÆ**

**1. Ëá™ÂÆö‰πâ StreamToolCallChecker**

```go
func customStreamToolCallChecker(ctx context.Context, sr *schema.StreamReader[*schema.Message]) (bool, error) {
    defer sr.Close()
    for {
        msg, err := sr.Recv()
        if err != nil {
            if errors.Is(err, io.EOF) {
                break
            }
            return false, err
        }

        if len(msg.ToolCalls) > 0 {
            return true, nil
        }
    }
    return false, nil
}

// Âú®ÂàõÂª∫ Host Êó∂‰ΩøÁî®
host := &host.Host{
    ChatModel:              chatModel,
    SystemPrompt:           "Your system prompt",
    StreamToolCallChecker:  customStreamToolCallChecker,
}
```

1. **ÈÖçÁΩÆ Summarizer**
ÂΩì Host ÂêåÊó∂ÈÄâÊã©Â§ö‰∏™‰∏ìÂÆ∂Êó∂ÔºåÈúÄË¶Å Summarizer Êù•Ê±áÊÄªÁªìÊûúÔºö

```go
hostMA, err := host.NewMultiAgent(ctx, &host.MultiAgentConfig{
    Host: *h,
    Specialists: []*host.Specialist{
        writer,
        reader,
        answerer,
    },
    Summarizer: &host.Summarizer{
        ChatModel:    summarizerModel,
        SystemPrompt: "Summarize the outputs from multiple specialists into a coherent response.",
    },
})
```





## ‰∫å„ÄÅeino assistant





## ‰∏â„ÄÅdeer-go

> [https://mp.weixin.qq.com/s/wT-UqAGxxJ0-h-zDqVXSSQ](https://mp.weixin.qq.com/s/wT-UqAGxxJ0-h-zDqVXSSQ)





## Âõõ„ÄÅmanus



